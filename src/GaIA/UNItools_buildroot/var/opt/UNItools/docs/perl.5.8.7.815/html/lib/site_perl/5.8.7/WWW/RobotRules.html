<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN">
<html>
<head>
<!-- saved from url=(0017)http://localhost/ -->
<script language="JavaScript" src="../../../../displayToc.js"></script>
<script language="JavaScript" src="../../../../tocParas.js"></script>
<script language="JavaScript" src="../../../../tocTab.js"></script>
<title>WWW::RobotRules - database of robots.txt-derived permissions</title>
<link rel="stylesheet" href="../../../../Active.css" type="text/css" />
<link rev="made" href="mailto:support@ActiveState.com" />
</head>

<body>

<script>writelinks('__top__',4);</script>
<h1><a>WWW::RobotRules - database of robots.txt-derived permissions</a></h1>
<p><a name="__index__"></a></p>

<!-- INDEX BEGIN -->

<ul>

	<li><a href="#name">NAME</a></li>
	<li><a href="#synopsis">SYNOPSIS</a></li>
	<li><a href="#description">DESCRIPTION</a></li>
	<li><a href="#robots_txt">ROBOTS.TXT</a></li>
	<li><a href="#robots_txt_examples">ROBOTS.TXT EXAMPLES</a></li>
	<li><a href="#see_also">SEE ALSO</a></li>
</ul>
<!-- INDEX END -->

<hr />
<p>
</p>
<h1><a name="name">NAME</a></h1>
<p>WWW::RobotRules - database of robots.txt-derived permissions</p>
<p>
</p>
<hr />
<h1><a name="synopsis">SYNOPSIS</a></h1>
<pre>
 use WWW::RobotRules;
 my $rules = WWW::RobotRules-&gt;new('MOMspider/1.0');</pre>
<pre>
 use LWP::Simple qw(get);</pre>
<pre>
 {
   my $url = &quot;<a href="http://some.place/robots.txt&quot">http://some.place/robots.txt&quot</a>;;
   my $robots_txt = get $url;
   $rules-&gt;parse($url, $robots_txt) if defined $robots_txt;
 }</pre>
<pre>
 {
   my $url = &quot;<a href="http://some.other.place/robots.txt&quot">http://some.other.place/robots.txt&quot</a>;;
   my $robots_txt = get $url;
   $rules-&gt;parse($url, $robots_txt) if defined $robots_txt;
 }</pre>
<pre>
 # Now we can check if a URL is valid for those servers
 # whose &quot;robots.txt&quot; files we've gotten and parsed:
 if($rules-&gt;allowed($url)) {
     $c = get $url;
     ...
 }</pre>
<p>
</p>
<hr />
<h1><a name="description">DESCRIPTION</a></h1>
<p>This module parses <em>/robots.txt</em> files as specified in
``A Standard for Robot Exclusion'', at
&lt;http://www.robotstxt.org/wc/norobots.html&gt;
Webmasters can use the <em>/robots.txt</em> file to forbid conforming
robots from accessing parts of their web site.</p>
<p>The parsed files are kept in a WWW::RobotRules object, and this object
provides methods to check if access to a given URL is prohibited.  The
same WWW::RobotRules object can be used for one or more parsed
<em>/robots.txt</em> files on any number of hosts.</p>
<p>The following methods are provided:</p>
<dl>
<dt><strong><a name="item_new">$rules = WWW::RobotRules-&gt;<code>new($robot_name)</code></a></strong>

<dd>
<p>This is the constructor for WWW::RobotRules objects.  The first
argument given to <a href="#item_new"><code>new()</code></a> is the name of the robot.</p>
</dd>
</li>
<dt><strong><a name="item_parse">$rules-&gt;parse($robot_txt_url, $content, $fresh_until)</a></strong>

<dd>
<p>The <a href="#item_parse"><code>parse()</code></a> method takes as arguments the URL that was used to
retrieve the <em>/robots.txt</em> file, and the contents of the file.</p>
</dd>
</li>
<dt><strong><a name="item_allowed">$rules-&gt;<code>allowed($uri)</code></a></strong>

<dd>
<p>Returns TRUE if this robot is allowed to retrieve this URL.</p>
</dd>
</li>
<dt><strong><a name="item_agent">$rules-&gt;<code>agent([$name])</code></a></strong>

<dd>
<p>Get/set the agent name. NOTE: Changing the agent name will clear the robots.txt
rules and expire times out of the cache.</p>
</dd>
</li>
</dl>
<p>
</p>
<hr />
<h1><a name="robots_txt">ROBOTS.TXT</a></h1>
<p>The format and semantics of the ``/robots.txt'' file are as follows
(this is an edited abstract of
&lt;http://www.robotstxt.org/wc/norobots.html&gt; ):</p>
<p>The file consists of one or more records separated by one or more
blank lines. Each record contains lines of the form</p>
<pre>
  &lt;field-name&gt;: &lt;value&gt;</pre>
<p>The field name is case insensitive.  Text after the '#' character on a
line is ignored during parsing.  This is used for comments.  The
following &lt;field-names&gt; can be used:</p>
<dl>
<dt><strong><a name="item_user_2dagent">User-Agent</a></strong>

<dd>
<p>The value of this field is the name of the robot the record is
describing access policy for.  If more than one <em>User-Agent</em> field is
present the record describes an identical access policy for more than
one robot. At least one field needs to be present per record.  If the
value is '*', the record describes the default access policy for any
robot that has not not matched any of the other records.</p>
</dd>
</li>
<dt><strong><a name="item_disallow">Disallow</a></strong>

<dd>
<p>The value of this field specifies a partial URL that is not to be
visited. This can be a full path, or a partial path; any URL that
starts with this value will not be retrieved</p>
</dd>
</li>
</dl>
<p>
</p>
<hr />
<h1><a name="robots_txt_examples">ROBOTS.TXT EXAMPLES</a></h1>
<p>The following example ``/robots.txt'' file specifies that no robots
should visit any URL starting with ``/cyberworld/map/'' or ``/tmp/'':</p>
<pre>
  User-agent: *
  Disallow: /cyberworld/map/ # This is an infinite virtual URL space
  Disallow: /tmp/ # these will soon disappear</pre>
<p>This example ``/robots.txt'' file specifies that no robots should visit
any URL starting with ``/cyberworld/map/'', except the robot called
``cybermapper'':</p>
<pre>
  User-agent: *
  Disallow: /cyberworld/map/ # This is an infinite virtual URL space</pre>
<pre>
  # Cybermapper knows where to go.
  User-agent: cybermapper
  Disallow:</pre>
<p>This example indicates that no robots should visit this site further:</p>
<pre>
  # go away
  User-agent: *
  Disallow: /</pre>
<p>
</p>
<hr />
<h1><a name="see_also">SEE ALSO</a></h1>
<p><a href="../../../../lib/site_perl/5.8.7/LWP/RobotUA.html">the LWP::RobotUA manpage</a>, <a href="../../../../lib/site_perl/5.8.7/WWW/RobotRules/AnyDBM_File.html">the WWW::RobotRules::AnyDBM_File manpage</a>
</p>

</body>

</html>
